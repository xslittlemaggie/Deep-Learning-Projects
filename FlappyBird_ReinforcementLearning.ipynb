{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FlappyBird_ReinforcementLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xslittlemaggie/Deep-Learning-Projects/blob/master/FlappyBird_ReinforcementLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "symHKiDfYnWJ",
        "colab_type": "text"
      },
      "source": [
        "## Step 0: Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5diya8iaB10",
        "colab_type": "text"
      },
      "source": [
        "### 1. Import the pygame environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX4ou8AHZ2EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "f497ca91-f44b-4ffa-8c45-6b6eef408f81"
      },
      "source": [
        "import os\n",
        "!git clone https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
        "os.chdir('PyGame-Learning-Environment')\n",
        "!pip install -e .\n",
        "!pip install pygame\n",
        "os.chdir('/content')\n",
        "\n",
        "import pygame"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PyGame-Learning-Environment'...\n",
            "remote: Enumerating objects: 1118, done.\u001b[K\n",
            "remote: Total 1118 (delta 0), reused 0 (delta 0), pack-reused 1118\u001b[K\n",
            "Receiving objects: 100% (1118/1118), 8.06 MiB | 31.37 MiB/s, done.\n",
            "Resolving deltas: 100% (592/592), done.\n",
            "Obtaining file:///content/PyGame-Learning-Environment\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ple==0.0.1) (1.16.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from ple==0.0.1) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->ple==0.0.1) (0.46)\n",
            "Installing collected packages: ple\n",
            "  Running setup.py develop for ple\n",
            "Successfully installed ple\n",
            "Collecting pygame\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/ede6428359f913ed9cd1643dd5533aefeb5a2699cc95bea089de50ead586/pygame-1.9.6-cp36-cp36m-manylinux1_x86_64.whl (11.4MB)\n",
            "\u001b[K     |████████████████████████████████| 11.4MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-1.9.6\n",
            "pygame 1.9.6\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGUS7dDCZ-7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU56af6EaTqR",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Load data from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F-kGk_qaSAd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "57fae5d9-55f4-450e-8e82-74042b2071d7"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOqG1CYtaw9o",
        "colab_type": "text"
      },
      "source": [
        "### 1. Copy data from google drive to colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj9w1MmIatQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy data from google drive to Colab\n",
        "!cp -r \"/content/drive/My Drive/Deep Learning projects/FlappyBird\" \"/content\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiobNzEOYobH",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Define the flappy_bird_utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtR4egobbc84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load():\n",
        "  \n",
        "  # path of background\n",
        "  background_path = \"/content/FlappyBird/assets/sprites/background-black.png\"\n",
        "  \n",
        "  # path of pipe\n",
        "  pipe_path = \"/content/FlappyBird/assets/sprites/pipe-green.png\"\n",
        "  \n",
        "  images, hitmasks = {}, {}\n",
        "\n",
        "  # numbers of sprites for score display\n",
        "  images[\"numbers\"] = (\n",
        "      pygame.image.load(\"/content/FlappyBird/assets/sprites/0.png\").convert_alpha(),\n",
        "      pygame.image.load(\"/content/FlappyBird/assets/sprites/1.png\").convert_alpha(),\n",
        "      pygame.image.load(\"/content/FlappyBird/assets/sprites/2.png\").convert_alpha(),\n",
        "      pygame.image.load(\"/content/FlappyBird/assets/sprites/3.png\").convert_alpha(),\n",
        "      pygame.image.load(\"/content/FlappyBird/assets/sprites/4.png\").convert_alpha(),\n",
        "      pygame.image.load(\"/content/FlappyBird/assets/sprites/5.png\").convert_alpha(),\n",
        "      pygame.image.load(\"/content/FlappyBird/assets/sprites/6.png\").convert_alpha(),\n",
        "      pygame.image.load(\"/content/FlappyBird/assets/sprites/7.png\").convert_alpha(),\n",
        "      pygame.image.load(\"/content/FlappyBird/assets/sprites/8.png\").convert_alpha(),\n",
        "      pygame.image.load(\"/content/FlappyBird/assets/sprites/9.png\").convert_alpha()\n",
        "  )\n",
        "  \n",
        "  # base  \n",
        "  images[\"base\"] = pygame.image.load(\"/content/FlappyBird/assets/sprites/base.png\").convert_alpha()\n",
        "  \n",
        "  # select random background sprites\n",
        "  images['background'] = pygame.image.load(background_path).convert()\n",
        "    \n",
        "    \n",
        "  # select random player sprites\n",
        "  images[\"player\"] = (\n",
        "      pygame.image.load(\"/content/FlappyBird/assets/sprites/redbird-upflap.png\").convert_alpha(),\n",
        "      pygame.image.load(\"/content/FlappyBird/assets/sprites/redbird-midflap.png\").convert_alpha(),\n",
        "      pygame.image.load(\"/content/FlappyBird/assets/sprites/redbird-downflap.png\").convert_alpha()   \n",
        "  )\n",
        "  \n",
        "  images[\"pipe\"] = (\n",
        "    pygame.transform.rotate(\n",
        "      pygame.image.load(pipe_path).convert_alpha(), 180),   \n",
        "    pygame.image.load(pipe_path).convert_alpha()\n",
        "  )\n",
        "  \n",
        "  \n",
        "  # hitmask for pipes\n",
        "  hitmasks[\"pipe\"] = (\n",
        "      getHitmask(images[\"pipe\"][0]),\n",
        "      getHitmask(images[\"pipe\"][1]),\n",
        "  )\n",
        "  \n",
        "  # hitmask for player\n",
        "  hitmasks['player'] = (\n",
        "      getHitmask(images['player'][0]),\n",
        "      getHitmask(images['player'][1]),\n",
        "      getHitmask(images['player'][2]),\n",
        "  )\n",
        "    \n",
        "  return images, hitmasks\n",
        "\n",
        "def getHitmask(image):\n",
        "  mask = []\n",
        "  for x in range(image.get_width()):\n",
        "    mask.append([])\n",
        "    for y in range(image.get_height()):\n",
        "      mask[x].append(bool(image.get_at((x, y))[3]))\n",
        "      \n",
        "  return mask\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EMeHm0efY0M",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Define the wrapped_flappy_bird"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIjfyt9Je1a7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pygame.surfarray as surfarray\n",
        "from pygame.locals import *\n",
        "from itertools import cycle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCVBR9qSeePQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "d0e53b79-30b0-49d2-e08f-71ebcf3163f1"
      },
      "source": [
        "fps = 30\n",
        "screenwidth = 288\n",
        "screenheight = 512\n",
        "\n",
        "pygame.init()\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "\n",
        "fpsclock = pygame.time.Clock()\n",
        "screen = pygame.display.set_mode((screenwidth, screenheight))\n",
        "pygame.display.set_caption('Flappy Bird')\n",
        "\n",
        "images, hitmasks = load()\n",
        "images"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'background': <Surface(288x512x8 SW)>,\n",
              " 'base': <Surface(336x112x32 SW)>,\n",
              " 'numbers': (<Surface(24x36x32 SW)>,\n",
              "  <Surface(16x36x32 SW)>,\n",
              "  <Surface(24x36x32 SW)>,\n",
              "  <Surface(24x36x32 SW)>,\n",
              "  <Surface(24x36x32 SW)>,\n",
              "  <Surface(24x36x32 SW)>,\n",
              "  <Surface(24x36x32 SW)>,\n",
              "  <Surface(24x36x32 SW)>,\n",
              "  <Surface(24x36x32 SW)>,\n",
              "  <Surface(24x36x32 SW)>),\n",
              " 'pipe': (<Surface(52x320x32 SW)>, <Surface(52x320x32 SW)>),\n",
              " 'player': (<Surface(34x24x32 SW)>,\n",
              "  <Surface(34x24x32 SW)>,\n",
              "  <Surface(34x24x32 SW)>)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywKgCxl8h4fu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "d88e6ec9-f3b0-46c5-ef78-90ca0cfce238"
      },
      "source": [
        "pipeGapSize = 100 # gap between upper and lower part of pipe\n",
        "basey = screenheight * 0.79\n",
        "\n",
        "# get the size of the player\n",
        "player_width = images[\"player\"][0].get_width()\n",
        "player_height = images[\"player\"][0].get_height()\n",
        "print(\"The width of the player: {}\".format(player_width))\n",
        "print(\"The height of the player: {}\".format(player_height))\n",
        "print()\n",
        "\n",
        "# get the size of the pipe\n",
        "pipe_width = images[\"pipe\"][0].get_width()\n",
        "pipe_height = images[\"pipe\"][0].get_height()\n",
        "print(\"The width of the pipe: {}\".format(pipe_width))\n",
        "print(\"The height of the pipe: {}\".format(pipe_height))\n",
        "print()\n",
        "\n",
        "# get the size of the background\n",
        "background_width = images[\"background\"].get_width()\n",
        "background_height = images[\"background\"].get_height()\n",
        "print(\"The width of the background: {}\".format(background_width))\n",
        "print(\"The height of the background: {}\".format(background_height))\n",
        "\n",
        "player_index_gen = cycle([0, 1, 2, 1])"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The width of the player: 34\n",
            "The height of the player: 24\n",
            "\n",
            "The width of the pipe: 52\n",
            "The height of the pipe: 320\n",
            "\n",
            "The width of the background: 288\n",
            "The height of the background: 512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9gd4aGHkqZY",
        "colab_type": "text"
      },
      "source": [
        "### 1. define GameState"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXo97CQSkp8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GameState:\n",
        "  def __init__(self):\n",
        "    self.score = self.playerIndex = self.loopIter = 0\n",
        "    self.playerx = int(screenwidth * 0.2)\n",
        "    self.playery = int((screenheight - player_height)/2)  # set the start position\n",
        "    self.basex = 0\n",
        "    self.baseShift = images[\"base\"].get_width() - background_width\n",
        "    \n",
        "    newPipe1 = getRandomPipe()\n",
        "    newPipe2 = getRandomPipe()\n",
        "    self.upperPipes = [\n",
        "        {\"x\": screenwidth, \"y\": newPipe1[0][\"y\"]},\n",
        "        {\"x\": screenwidth + (screenwidth/2), \"y\": newPipe2[0][\"y\"]},\n",
        "    ]\n",
        "    \n",
        "    self.lowerPipes = [\n",
        "        {\"x\": screenwidth, \"y\": newPipe1[1][\"y\"]},\n",
        "        {\"x\": screenwidth + (screenwidth/2), \"y\": newPipe2[1][\"y\"]},        \n",
        "    ]\n",
        "    \n",
        "    # player velocity, max velocity, downward accleration, accleration on flap\n",
        "    self.pipeVelX = -4\n",
        "    self.playerVelY    =  0    # player's velocity along Y, default same as playerFlapped\n",
        "    self.playerMaxVelY =  10   # max vel along Y, max descend speed\n",
        "    self.playerMinVelY =  -8   # min vel along Y, max ascend speed\n",
        "    self.playerAccY    =   1   # players downward accleration\n",
        "    self.playerFlapAcc =  -9   # players speed on flapping\n",
        "    self.playerFlapped = False # True when player flaps\n",
        "    \n",
        "    \n",
        "  def frame_step(self, input_actions):\n",
        "    pygame.event.pump()\n",
        "    \n",
        "    reward = 0.1\n",
        "    terminal = False\n",
        "    \n",
        "    if sum(input_actions) != 1:\n",
        "      raise ValueError(\"Multiple input actions!\")\n",
        "      \n",
        "    # input_actions[0] == 1: do nothing\n",
        "    # input_actions[1] == 1: flap the bird\n",
        "    if input_actions[1] == 1:\n",
        "        if self.playery > -2 * player_height:\n",
        "            self.playerVelY = self.playerFlapAcc\n",
        "            self.playerFlapped = True\n",
        "   \n",
        "    # check for score\n",
        "    playerMidPos = self.playerx + player_width / 2\n",
        "    for pipe in self.upperPipes:\n",
        "        pipeMidPos = pipe['x'] + pipe_width/ 2\n",
        "        if pipeMidPos <= playerMidPos < pipeMidPos + 4:\n",
        "            self.score += 1\n",
        "            reward = 1\n",
        "            \n",
        "            \n",
        "    # playerIndex basex change\n",
        "    if (self.loopIter + 1) % 3 == 0:\n",
        "        self.playerIndex = next(player_index_gen)\n",
        "    self.loopIter = (self.loopIter + 1) % 30\n",
        "    self.basex = -((-self.basex + 100) % self.baseShift)\n",
        "    \n",
        "    # player's movement\n",
        "    if self.playerVelY < self.playerMaxVelY and not self.playerFlapped:\n",
        "        self.playerVelY += self.playerAccY\n",
        "    if self.playerFlapped:\n",
        "        self.playerFlapped = False\n",
        "    self.playery += min(self.playerVelY, basey - self.playery - player_height)\n",
        "    if self.playery < 0:\n",
        "        self.playery = 0\n",
        "        \n",
        "        \n",
        "    # move pipes to left\n",
        "    for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
        "        uPipe['x'] += self.pipeVelX\n",
        "        lPipe['x'] += self.pipeVelX\n",
        "        \n",
        "    # add new pipe when first pipe is about to touch left of screen\n",
        "    if 0 < self.upperPipes[0]['x'] < 5:\n",
        "        newPipe = getRandomPipe()\n",
        "        self.upperPipes.append(newPipe[0])\n",
        "        self.lowerPipes.append(newPipe[1])\n",
        "        \n",
        "    # remove first pipe if its out of the screen\n",
        "    if self.upperPipes[0]['x'] < -pipe_width:\n",
        "        self.upperPipes.pop(0)\n",
        "        self.lowerPipes.pop(0)\n",
        "        \n",
        "    # check if crash here\n",
        "    isCrash= checkCrash({'x': self.playerx, 'y': self.playery,\n",
        "                         'index': self.playerIndex},\n",
        "                        self.upperPipes, self.lowerPipes)\n",
        "    if isCrash:\n",
        "        #SOUNDS['hit'].play()\n",
        "        #SOUNDS['die'].play()\n",
        "        terminal = True\n",
        "        self.__init__()\n",
        "        reward = -1\n",
        "        \n",
        "    # draw sprites\n",
        "    screen.blit(images['background'], (0,0))\n",
        "    \n",
        "    for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
        "        screen.blit(images['pipe'][0], (uPipe['x'], uPipe['y']))\n",
        "        screen.blit(images['pipe'][1], (lPipe['x'], lPipe['y']))\n",
        "\n",
        "        screen.blit(images['base'], (self.basex, basey))\n",
        "        # print score so player overlaps the score\n",
        "        # showScore(self.score)\n",
        "        screen.blit(images['player'][self.playerIndex],\n",
        "                    (self.playerx, self.playery))\n",
        "\n",
        "    image_data = pygame.surfarray.array3d(pygame.display.get_surface())\n",
        "    pygame.display.update()\n",
        "    #print (\"FPS\" , fpsclock.get_fps())\n",
        "    fpsclock.tick(fps)\n",
        "    #print self.upperPipes[0]['y'] + pipe_height - int(basey * 0.2)\n",
        "    return image_data, reward, terminal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwuuf8D1opyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getRandomPipe():\n",
        "    \"\"\"returns a randomly generated pipe\"\"\"\n",
        "    # y of gap between upper and lower pipe\n",
        "    gapYs = [20, 30, 40, 50, 60, 70, 80, 90]\n",
        "    index = random.randint(0, len(gapYs)-1)\n",
        "    gapY = gapYs[index]\n",
        "\n",
        "    gapY += int(basey * 0.2)\n",
        "    pipeX = screenwidth + 10\n",
        "\n",
        "    return [\n",
        "        {'x': pipeX, 'y': gapY - pipe_height},  # upper pipe\n",
        "        {'x': pipeX, 'y': gapY + pipeGapSize},  # lower pipe\n",
        "    ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zZZhi5WovLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def showScore(score):\n",
        "    \"\"\"displays score in center of screen\"\"\"\n",
        "    scoreDigits = [int(x) for x in list(str(score))]\n",
        "    totalWidth = 0 # total width of all numbers to be printed\n",
        "\n",
        "    for digit in scoreDigits:\n",
        "        totalWidth += images['numbers'][digit].get_width()\n",
        "\n",
        "    Xoffset = (screenwidth - totalWidth) / 2\n",
        "\n",
        "    for digit in scoreDigits:\n",
        "        screen.blit(images['numbers'][digit], (Xoffset, screenheight * 0.1))\n",
        "        Xoffset += images['numbers'][digit].get_width()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rISvgEIio674",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def checkCrash(player, upperPipes, lowerPipes):\n",
        "    \"\"\"returns True if player collders with base or pipes.\"\"\"\n",
        "    pi = player['index']\n",
        "    player['w'] = images['player'][0].get_width()\n",
        "    player['h'] = images['player'][0].get_height()\n",
        "\n",
        "    # if player crashes into ground\n",
        "    if player['y'] + player['h'] >= basey - 1:\n",
        "        return True\n",
        "    else:\n",
        "\n",
        "        playerRect = pygame.Rect(player['x'], player['y'],\n",
        "                      player['w'], player['h'])\n",
        "\n",
        "        for uPipe, lPipe in zip(upperPipes, lowerPipes):\n",
        "            # upper and lower pipe rects\n",
        "            uPipeRect = pygame.Rect(uPipe['x'], uPipe['y'], pipe_width, pipe_height)\n",
        "            lPipeRect = pygame.Rect(lPipe['x'], lPipe['y'], pipe_width, pipe_height)\n",
        "\n",
        "            # player and upper/lower pipe hitmasks\n",
        "            pHitMask = hitmasks['player'][pi]\n",
        "            uHitmask = hitmasks['pipe'][0]\n",
        "            lHitmask = hitmasks['pipe'][1]\n",
        "\n",
        "            # if bird collided with upipe or lpipe\n",
        "            uCollide = pixelCollision(playerRect, uPipeRect, pHitMask, uHitmask)\n",
        "            lCollide = pixelCollision(playerRect, lPipeRect, pHitMask, lHitmask)\n",
        "\n",
        "            if uCollide or lCollide:\n",
        "                return True\n",
        "\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wHzarNNpSdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pixelCollision(rect1, rect2, hitmask1, hitmask2):\n",
        "    \"\"\"Checks if two objects collide and not just their rects\"\"\"\n",
        "    rect = rect1.clip(rect2)\n",
        "\n",
        "    if rect.width == 0 or rect.height == 0:\n",
        "        return False\n",
        "\n",
        "    x1, y1 = rect.x - rect1.x, rect.y - rect1.y\n",
        "    x2, y2 = rect.x - rect2.x, rect.y - rect2.y\n",
        "\n",
        "    for x in range(rect.width):\n",
        "        for y in range(rect.height):\n",
        "            if hitmask1[x1+x][y1+y] and hitmask2[x2+x][y2+y]:\n",
        "                return True\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nkvHoUmpeYE",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka3v_Jglo6Xv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#transform: 几何变换或其它变换，如旋转、拉伸和拉东变换等\n",
        "#color: 颜色空间变换\n",
        "# exposure: 图片强度调整，如亮度调整、直方图均衡等\n",
        "import skimage\n",
        "from skimage import transform, color, exposure \n",
        "from skimage.transform import rotate\n",
        "from skimage.viewer import ImageViewer\n",
        "import sys\n",
        "sys.path.append(\"game/\")\n",
        "#import wrapped_flappy_bird as game\n",
        "#import random\n",
        "#import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "import json\n",
        "from keras.initializers import normal, identity\n",
        "from keras.models import model_from_json\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import SGD , Adam\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAmvk14Kp6M4",
        "colab_type": "text"
      },
      "source": [
        "## 参数初始化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYRUwjyhpyLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GAME = 'bird' # the name of the game being played for log files\n",
        "CONFIG = 'nothreshold'\n",
        "ACTIONS = 2 # number of valid actions\n",
        "    # 0: 表示不flap\n",
        "    # 1: 表示flap\n",
        "GAMMA = 0.99 # decay rate of past observations\n",
        "OBSERVATION = 3200. # timesteps to observe before training\n",
        "EXPLORE = 3000000. # frames over which to anneal epsilon\n",
        "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
        "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
        "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
        "BATCH = 32 # size of minibatch\n",
        "FRAME_PER_ACTION = 1\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "img_rows , img_cols = 80, 80\n",
        "#Convert image into Black and white\n",
        "img_channels = 4 #We stack 4 frames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBOox29GqBvX",
        "colab_type": "text"
      },
      "source": [
        "## Step 2：建立CNN模型, 输入图片，输出动作概率\n",
        "\n",
        "Actions = 2 总共两种动作\n",
        "- 0：不拍打翅膀\n",
        "- 1: 拍打翅膀"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUinsv8Fp_S6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "5947637f-e98d-45ae-afc3-4a2ea13d9e7c"
      },
      "source": [
        "def buildmodel():\n",
        "    print(\"Now we build the model...\")\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (8, 8), strides=(4, 4), padding='same',input_shape=(img_rows,img_cols,img_channels)))  #80*80*4\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(2))\n",
        "   \n",
        "    adam = Adam(lr=LEARNING_RATE)\n",
        "    model.compile(loss='mse',optimizer=adam)\n",
        "    print(\"We finish building the model\")\n",
        "    return model\n",
        "model = buildmodel()\n",
        "model.summary()"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now we build the model...\n",
            "We finish building the model\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_61 (Conv2D)           (None, 20, 20, 32)        8224      \n",
            "_________________________________________________________________\n",
            "activation_81 (Activation)   (None, 20, 20, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_62 (Conv2D)           (None, 10, 10, 64)        32832     \n",
            "_________________________________________________________________\n",
            "activation_82 (Activation)   (None, 10, 10, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_63 (Conv2D)           (None, 10, 10, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_83 (Activation)   (None, 10, 10, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_21 (Flatten)         (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 512)               3277312   \n",
            "_________________________________________________________________\n",
            "activation_84 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 3,356,322\n",
            "Trainable params: 3,356,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYZPRBd7qEEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Step 3: 数据预处理"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX4xs0VdqHYc",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: 数据预处理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR48TipNqJDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSgIKa92qKt0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "09e5fc84-0064-4678-a9ac-cf51b1546a5d"
      },
      "source": [
        "pipe_path = \"/content/FlappyBird/assets/sprites/pipe-green.png\"\n",
        "plt.figure(figsize = (4, 4))\n",
        "plt.imshow(cv2.imread(pipe_path))\n",
        "plt.show()"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFEAAAD8CAYAAAAPDUgGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcEj8ckmqVsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainNetwork(model,args):\n",
        "    # open up a game state to communicate with emulator\n",
        "    game_state = GameState()\n",
        "\n",
        "    # store the previous observations in replay memory\n",
        "    D = deque()\n",
        "\n",
        "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
        "    do_nothing = np.zeros(ACTIONS)\n",
        "    do_nothing[0] = 1\n",
        "    x_t, r_0, terminal = game_state.frame_step(do_nothing)\n",
        "\n",
        "    x_t = skimage.color.rgb2gray(x_t)\n",
        "    x_t = skimage.transform.resize(x_t,(80,80))\n",
        "    x_t = skimage.exposure.rescale_intensity(x_t,out_range=(0,255))\n",
        "\n",
        "    x_t = x_t / 255.0\n",
        "\n",
        "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
        "    #print (s_t.shape)\n",
        "\n",
        "    #In Keras, need to reshape\n",
        "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*80*80*4\n",
        "\n",
        "    \n",
        "    if args['mode'] == 'Run':\n",
        "        OBSERVE = 999999999    #We keep observe, never train\n",
        "        epsilon = FINAL_EPSILON\n",
        "        print (\"Now we load weight\")\n",
        "        model.load_weights(\"model.h5\")\n",
        "        adam = Adam(lr=LEARNING_RATE)\n",
        "        model.compile(loss='mse',optimizer=adam)\n",
        "        print (\"Weight load successfully\")    \n",
        "    else:                       #We go to training mode\n",
        "        OBSERVE = OBSERVATION\n",
        "        epsilon = INITIAL_EPSILON\n",
        "\n",
        "    t = 0\n",
        "    while (True):\n",
        "        loss = 0\n",
        "        Q_sa = 0\n",
        "        action_index = 0\n",
        "        r_t = 0\n",
        "        a_t = np.zeros([ACTIONS])\n",
        "        #choose an action epsilon greedy\n",
        "        if t % FRAME_PER_ACTION == 0:\n",
        "            if random.random() <= epsilon:\n",
        "                print(\"----------Random Action----------\")\n",
        "                action_index = random.randrange(ACTIONS)\n",
        "                a_t[action_index] = 1\n",
        "            else:\n",
        "                q = model.predict(s_t)       #input a stack of 4 images, get the prediction\n",
        "                max_Q = np.argmax(q)\n",
        "                action_index = max_Q\n",
        "                a_t[max_Q] = 1\n",
        "\n",
        "        #We reduced the epsilon gradually\n",
        "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
        "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
        "\n",
        "        #run the selected action and observed next state and reward\n",
        "        x_t1_colored, r_t, terminal = game_state.frame_step(a_t)\n",
        "\n",
        "        x_t1 = skimage.color.rgb2gray(x_t1_colored)\n",
        "        x_t1 = skimage.transform.resize(x_t1,(80,80))\n",
        "        x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range=(0, 255))\n",
        "\n",
        "\n",
        "        x_t1 = x_t1 / 255.0\n",
        "\n",
        "\n",
        "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x80x80x1\n",
        "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
        "\n",
        "        # store the transition in D\n",
        "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
        "        if len(D) > REPLAY_MEMORY:\n",
        "            D.popleft()\n",
        "\n",
        "        #only train if done observing\n",
        "        if t > OBSERVE:\n",
        "            #sample a minibatch to train on\n",
        "            minibatch = random.sample(D, BATCH)\n",
        "\n",
        "            #Now we do the experience replay\n",
        "            state_t, action_t, reward_t, state_t1, terminal = zip(*minibatch)\n",
        "            state_t = np.concatenate(state_t)\n",
        "            state_t1 = np.concatenate(state_t1)\n",
        "            targets = model.predict(state_t)\n",
        "            Q_sa = model.predict(state_t1)\n",
        "            targets[range(BATCH), action_t] = reward_t + GAMMA*np.max(Q_sa, axis=1)*np.invert(terminal)\n",
        "\n",
        "            loss += model.train_on_batch(state_t, targets)\n",
        "\n",
        "        s_t = s_t1\n",
        "        t = t + 1\n",
        "\n",
        "        # save progress every 10000 iterations\n",
        "        if t % 1000 == 0:\n",
        "            print(\"Now we save model\")\n",
        "            model.save_weights(\"model.h5\", overwrite=True)\n",
        "            with open(\"model.json\", \"w\") as outfile:\n",
        "                json.dump(model.to_json(), outfile)\n",
        "\n",
        "        # print info\n",
        "        state = \"\"\n",
        "        if t <= OBSERVE:\n",
        "            state = \"observe\"\n",
        "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
        "            state = \"explore\"\n",
        "        else:\n",
        "            state = \"train\"\n",
        "\n",
        "        print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n",
        "            \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \\\n",
        "            \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
        "\n",
        "    print(\"Episode finished!\")\n",
        "    print(\"************************\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvkwlf4YrIDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def playGame(args):\n",
        "    model = buildmodel()\n",
        "    trainNetwork(model,args)\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Description of your program')\n",
        "    parser.add_argument('-m','--mode', help='Train / Run', required=True)\n",
        "    args = vars(parser.parse_args())\n",
        "    playGame(args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaCe8-5JrKiO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4888fcea-a94d-4645-b16a-a2c4e12201e1"
      },
      "source": [
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.Session(config=config)\n",
        "from keras import backend as K\n",
        "K.set_session(sess)\n",
        "args = {\"mode\":\"train\"}\n",
        "playGame(args)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now we build the model...\n",
            "We finish building the model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/exposure/exposure.py:351: RuntimeWarning: invalid value encountered in true_divide\n",
            "  image = (image - imin) / float(imax - imin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 122 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 123 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 124 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 125 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 126 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 127 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 128 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 129 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 130 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 131 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 132 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 133 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 134 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 135 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 136 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 137 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 138 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 139 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 140 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 141 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 142 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 143 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 144 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 145 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-183-d54a0ff3754a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplayGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-181-1b5c7ad39412>\u001b[0m in \u001b[0;36mplayGame\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplayGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrainNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-180-02ca51784618>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m#run the selected action and observed next state and reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mx_t1_colored\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mx_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb2gray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t1_colored\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-169-5c951646feed>\u001b[0m in \u001b[0;36mframe_step\u001b[0;34m(self, input_actions)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m#print (\"FPS\" , fpsclock.get_fps())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mfpsclock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0;31m#print self.upperPipes[0]['y'] + pipe_height - int(basey * 0.2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faZ1b3HIrMVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}